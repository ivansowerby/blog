# Artificial Sentience

<div class="giphy"><iframe id="QidsMkCa1AivKWEdQN" src="https://giphy.com/embed/QidsMkCa1AivKWEdQN" alt="art-loop-glitch"></iframe></div>

> Thoughts on sentience

#### By Ivan Sowerby

---

## AGI?

Framed as the *final* invention of *man* - AGI, still lacks agreed definition. Over time, accepted terminology has drowned with both:
* Unexpected **developments**, and **push-backs**.
* (Understandably) **Fantasised** hype/interest over such fundamental change.

Marketing tactics have already diluted the term - AI (Artificial Intelligence) with such a ambiguous definition. Both by companies and public media, such that most modern programs can be considered "intelligent".

Arguably, there is some sense to this, if we take a [reductionist](https://wikipedia.org/wiki/Reductionism)-[computationalism](https://wikipedia.org/wiki/Computational_theory_of_mind) approach. If *many* neurons can form intelligence (and *liveliness*), then a singular neuron must be a building-block of intelligence (going against an unlikely binary, i.e. intelligent, or not).

With AI being generalised to specific smokescreen **mask**(s) of intelligence, merely *pretending* - even if a weak argument against , given we do not truly yet understand the tapestry of intelligence itself. We instead consider AGI, however there is a starkly **binary** opinion to this - which I believe to be dependent on **timelines** and **takeoffs**, along with a trickle of game theory.

To name a few: [Sam Altman](https://wikipedia.org/wiki/Sam_Altman), and [Andrej Karpathy](https://wikipedia.org/wiki/Andrej_Karpathy) both consider AGI to be an **autonomous** AI system. Capable of substituting the (vast) majority of human-level (mental) intelligence, especially economically - but theoretical capability does not necessarily mean immediate scale to population-wide automation.

This definition suggests a likely **longer timeline** (compared to my definition of AGI[^1]), and blurs the line between AGI and ASI[^2] as for a **harder takeoff**. What stops a **truly autonomous** system from recursive incremental improvements and so [intelligence explosion](https://wikipedia.org/wiki/Technological_singularity#Intelligence_explosion)? And is a single-machine capable of many peoples' *jobs* not necessarily super-intelligent? (i.e. many median AGIs[^2] approximate ASI[^2]). If true, likely amplifying the takeoff. 

To give some insight into the foresight at the forefront, I want to provide a chronological account of Sam Altman's positions (as of November 2024):
* October 2023 - Subscribes to a **short** timeline, **shallow** takeoff scenario with [his appearance on the Joe Rogan Experience](https://www.youtube.com/watch?v=7dCPytNTnjk).
* January 2024 - States at the [WEF annual meeting](https://www.popsci.com/technology/sam-altman-age-of-ai-will-require-an-energy-breakthrough/) that AGI "will change the world much less than we all think".
* March 2024 - Arguing on his [2nd appearance on the Lex Fridman podcast](https://www.youtube.com/watch?v=jvqFAi7vkBc):

> * Has GPT-4 *significantly* effected the global economy? Whereas GPT-n may be capable of.

> * Embodiment may not be necessary, but it would be *sad* to have an AGI, where humans have to action upon its purely-mental reasoning. Rasing questions with [Moravec's Paradox](http://wikipedia.org/wiki/Moravec%27s_paradox).

> * AI significantly accelerating scientific progress being the tipping-point (would that be automated?).

* September 2024 - In a [blog post](https://ia.samaltman.com/) titled The Intelligence Age, he suggests "It is possible that we will have superintelligence in a few thousand days".
* November 2024 - Claims to be excited *about* AGI in 2025 in a [conversation with Garry Tan of Y Combinator](https://www.youtube.com/watch?v=xXCBz_8hM9w&t=2772s).

While Sam Altman has kept his belief in a **short timeline** consistent, *his* takeoffs are rapidly steepening (**hard**) over just over a year. Of course skepticism is warranted, this goes against [reports of diminishing returns](https://www.theinformation.com/articles/openai-shifts-strategy-as-rate-of-gpt-ai-improvements-slows) which some (such as [Gary Marcus](https://wikipedia.org/wiki/Gary_Marcus)) adamantly believe, however I am more swayed towards the following [Tweet/X](https://x.com/polynoamial/status/1855037689533178289):

<div class="box"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;ve heard people claim that Sam is just drumming up hype, but from what I&#39;ve seen everything he&#39;s saying matches the ~median view of <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a> researchers on the ground. <a href="https://t.co/nd0itSdQLw">https://t.co/nd0itSdQLw</a></p>&mdash; Noam Brown (@polynoamial\) <a href="https://twitter.com/polynoamial/status/1855037689533178289?ref_src=twsrc%5Etfw">November 9, 2024</a></blockquote></div>

> Although arguably his timelines are contracting too, as even I was surprised by the comment on 2025 - even if vague (on the topic, I deeply respect Altman's [humility here](https://x.com/sama/status/1855100359511097828))

> Moreover, there is an argument to be made that the OpenAI board may decide to prematurely announce "AGI" due to a [reported clause in the Microsoft-OpenAI contract](https://techcrunch.com/2024/10/17/the-surprising-way-openai-could-get-out-of-its-pact-with-microsoft/) which states Microsoft would not gain access to such a model - this has been suggested over **alleged** disputes between the companies, that is Microsoft not delivering on *promised* compute and its senior leaders hastily demanding new models/features (possibly overlooking OpenAI's safety procedures). I would take these *leaks* with a grain of salt, given the odd social lucrativeness of AI *drama* - typical of humans ðŸ¤–.

As for my definition, I see AGI as an AI (*machine*) capable of performing (**cognitive**) tasks to the ability of a **median** (untrained) human[^2] - even if not necessarily autonomous yet. I agree with [Geoffrey Hinton](https://wikipedia.org/wiki/Geoffrey_Hinton)'s view that varying capabilities will (and have) become superhuman prior to **all** meeting human baseline (e.g. [Stockfish](https://stockfishchess.org/) and [AlphaGo](https://deepmind.google/research/breakthroughs/alphago/)) as he stated on [Bloomberg](https://www.bloomberg.com/news/videos/2024-11-12/robots-the-aliens-we-made-video), AGI as a strictly technical measurement is disadvised but nonetheless relevant to the industry and communication (such as in this blog); some tasks are computationally *easier* than others. Thus *easier* to achieve, implying a **shorter timeline** - although I must admit even this is relative. As a critique to myself - this is often shared with overtly-*optimistic* newcomers (in the 2020s, which likely includes myself).

As hopefully conveyed, timelines and takeoffs are not proportionally related - even if associations (correlations) can be found at opposing extremes (e.g. [lib-left and auth-right](https://wikipedia.org/wiki/The_Political_Compass), an obvious over-simplification but nonetheless similar by quadrants).

ASI[^2] however, has majority consensus on its definition - arguably because of its abstractness, its consequences intrinsically *unthinkable*. [Ray Kurzweil](https://wikipedia.org/wiki/Ray_Kurzweil) is a long-time thinker in this space who [Bill Gates](https://wikipedia.org/wiki/Bill_Gates) has described as "the best person I know at predicting the future of artificial intelligence", he predicts a takeoff ([technological singularity]()) in **2045** (or at least thereabouts).

> To add, Kurzweil has long predicted AGI's *arrival* in **2029** (holding this since the 90s, which drew criticism from peers at the time); public prediction markets largely agree with this timeline ([Manifold](https://manifold.markets/ai), and [Metaculus](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).

> I would like to acknowledge the late [Vernor Vinge](https://wikipedia.org/wiki/Vernor_Vinge) (1944-2024) here too, having spearheaded much of the early conversation on the [singularity](https://ntrs.nasa.gov/api/citations/19940022856/downloads/19940022856.pdf) early on - and although not in the scope of this blog, his appreciation for the wider *picture*: AI, internet, [BCI](https://wikipedia.org/wiki/Brain%E2%80%93computer_interface), [IA](https://wikipedia.org/wiki/Intelligence_amplification). Likely thanks to his Sci-Fi writing prowess.

### TL/DR

I take the latter, **median** AGI[^1] (of the average human) to be the definition for the [Onwards chapter](#onwards), leaning more into *conscious* ASI[^2] for the [Reason chapter](#reason).

---

## Onwards

To imagine (and create) AGI[^1] / ASI[^2] systems we need to decompose to fundamental components - even if this requires actively ignoring the *elephant in the room* (consciousness).

Each serves philosophical and or physical resolution to entangled problems with generalisation of intelligence: 

### Entropy

The beginning of the end; the closed system of the universes (net) tends from order to disorder - with the dissipation of energy through the (presumed) ever-expanding universe. Even if in local systems entropy decreases, it increases more so elsewhere.

Awakening, the essence of life is a story of thermodynamics due to the universal laws of entropy. Loosely evolving through these disorderly systems to form higher-level order - of greater and greater complexity. And so **accelerating** entropy increase. Inspired by intrinsically random quantum fluctuations and geometries derived from the symmetries of the universe - i.e. nature follows patterns at scale from non-uniform stochastic distributions, popularised publicly by the golden ratio. Due to enforcing optimisations in systems to materialise fittest solutions - given the environment it finds itself to be in.

From deep-sea hydrothermal vent primordial-soup, to the realisation (of illusive) *will* and *consciousness*. This procedural journey over billions of years suggests at intelligence and **sentience** being multi-dimensional spectrums/gradients - given selective environmental-pressures favour sentient fitness. Like our interface. For example, if consciousness **necessarily** won fitness - all states should tend to it eventually.

However, (as questioned by the Fermi paradox) the universe is seemingly lifelessly distilled, so either:
* The initial formation of this process is *very* rare.
* Consciousness does not *necessarily* win pay-off functions.

All culminating to the acknowledgement that immense computing clusters are required to train probabilstic SOTA AI systems by *fast-tracking* evolution (from orderly compressed human data input) - forcing the cost of energy (and our conceptual socioeconomic capital, by investment) to achieve greater magnitudes of *intelligence*. The universe by turn thermodynamically *favours* this devotion, as the latter penultimate invention (AGI[^1]) will expand the scope of energy consumption (post-scarcity) and dissipation. Giving rise to potential recursive improvements, given mind dualism is false - i.e. we are not special with our instance consciousness, this phenomena should thus be replicable.

This process can be imagined by a dart board, randomness in throws gives rise to mutations in score - however is guided to the (fittest) **highest** score, due to the player (the universe, and *her* entropy) favouring winning the game due to competitive instincts (intrinsic symmetries and conservations).

Machine sentience will be a perilous venture - but given heuristic-informed manipulation of NN[^4] architecture and entropy-controlled scaling (even if we do not understand the basis for consciousness) it is entirely possible to reach AGI[^1]. As we have: already reached an uncanny valley level of similarity, and are necessitating **at least** approximations of intelligence through vast human-encoded information. 


#### EBMs

...

### Interfaces

More recently (with the rise of LLMs[^3]), there has been intense debate over whether language (modality) can really *take you all the way* (to AGI[^1]), and possibly even beyond. Much of this work comes under the realm of reasoning.

Some argue (such as [Yann LeCun](https://wikipedia.org/wiki/Yann_LeCun)) that language is a *workaround* (albeit impressive), and although still critical, may need to be favoured for more *abstract* interfaces. I call these interfaces, as it is the modalities and so perceptual experiences that inform our ability to form complex thoughts that act as world models - including our inherited instinctual reflexes.

Language is a syntax of semantic lexicons - a modality of communication. Necessarily compressing abstract ideas as transferable models, to optimise mutual understanding. Acting heuristically, its efficiency lossy at best. The "proof is left as an exercise for the reader", **generative inference** is required on the recipients'-end to *fill in* conceptual blanks.

Presuming the refute of animism - as by lack of interface. For example: a rock has *no* interface(s), albeit a fascinating chemical structure - there is a fundamental lack of order for it to *meaningfully* interact back with us.

> However, it is amusing that (unintentional) self-inflicted injury from inanimate objects (by e.g. tripping on a rock) can enrage us to inflict *pain* back (kicking/punching) - as if to blame the universe. 

> When I say animism I am referring to the classical, *Western* spiritual-centric animism, where *everything* has a spiritual soul. I reject this notion, however I make the distinction as I believe [cyber animism](https://www.youtube.com/watch?v=34VOI_oo-qM) by [Joscha Bach](https://wikipedia.org/wiki/Joscha_Bach) has genuine insights to 'why life is the way it is' (especially with regards to self-organising software agents as an instance of self). 

Yann LeCun has given showing questions (after a row over inner monologue - and associated improvements to reasoning), demonstrating the limits of LLMs[^3]. Arguing LLMs[^3] to be an off-ramp on the tedious journey to human-level AI (I avoid AGI here, as LeCun claims that *general* intelligence is a false concept as even humans are not *general*). As of March 2024, no LLM[^3] I have prompted has successfully chose option D (zero-shot), from the following [Tweet/X](https://twitter.com/ylecun/status/1768353714794901530):

<div class="box"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">To people who claim that &quot;thinking and reasoning require language&quot;, here is a problem:<br>Imagine standing at the North Pole of the Earth.<br>Walk in any direction, in a straight line, for 1 km.<br>Now turn 90 degrees to the left.<br>Walk for as long as it takes to pass your starting point.<br>Have you walked:<br>1. More than 2xPi km<br>2. Exactly 2xPi km<br>3. Less than 2xPi km<br>4. I never came close to my starting point.<br><br>Think about how you tried to answer this question and tell us whether it was based on language.</p>&mdash; Yann LeCun (@ylecun\) <a href="https://twitter.com/ylecun/status/1768353714794901530?ref_src=twsrc%5Etfw">March 14, 2024</a></blockquote></div>

Even multi-modal models (e.g. GPT-4-turbo, and Claude-3-Opus), with SOTA image-modality fail at these [mental imagery](https://wikipedia.org/wiki/Mental_image)/visulisation problems. Modalities of interfaces **need** to cooperate. Language is a **meta-interface**, base interfaces (sound, vision, touch) work to influence/inform language processing.

This is **not** to say organisations/companies (respectively OpenAI, and Anthropic) have not achieved this *internally*. But rather attempts to outline problems with:
* Bridging modalities (to be more **generally** useful).
* Restricting bridging (i.e. to save compute - image modalities are only used when image input or generative output is prompted for).

> Shortly after my initial checkpoint upload of this blog, [OpenAI released their gpt-4o model](https://openai.com/index/hello-gpt-4o/), with the o in "4o" being an abbreviation for **omni** (the latin prefix for all/every) in reference to the *baked-in* wide breath of multi-modality: from text, to images (including autoregressive generation), to audio/voice, to 3D models (and I predict video in the near future too). I see this cross-domain generality to be essential to harness the full potential of [world simulating/modelling](https://openai.com/index/video-generation-models-as-world-simulators/) as later explained at the end of the [Reason chapter](#reason) - a key component to AGI as described by LeCun (although disagreeing with the effectiveness of video generators for this due to current hallucinations).

The aftermath of the withering gusts of the AI winter (up to the late 90s) deprived consensus - daylight on **symbolic AI** was dimming. **Connectionists**, however, recognised the (contemporary) belittled importance of ANNs[^5]. Accelerating progress has seen this being realised. Since the 2010s, LLMs[^3] have showcased these leaps in *abilities* - thanks to improvements in architecture (the [transformer](https://wikipedia.org/wiki/Transformer_(deep_learning_architecture))), and scaling (compute). These capabilities have already stunned many professionals in the field. 

### Guesswork

* Probabilistic/Stochastic/Stastical
* Hallucinations - "Dreams shed light on the dim places where reason itself has yet to voyage"
* Creation - "Creativity is just connecting things. When you ask creative people how they did something, they feel a little guilty because they didnâ€™t really do it, they just saw something"
* Quantum endeavours

### Reasoning

*Intelligence* in the human perception is often attributed to realisation of future states, imagining outcomes and perspectives, and lossly interpreting states.

* Step-by-step
* Actions-Events-Perceptions

### Attention

* In-weight adjustments
* Context-window

### Memories

* Long-term, beyond the context window (of inference)

### Branches (MoE)

* Generalisation by specialisation over functions
* Hierachical/Sparse connectivity

### Plasticity

* On the subject of memories

Digital Vs. Biological NNs
- Training, Inference

### Objective

* Ontology, meta-meaning (higher-order)
* Reinforcement, optimisation (the "happiness" of an NN)
* Will AI hide intention?

### Scaling

Elephants and Ostriches (*Is scaling all you need*?)
(Pre-)Training time and test time compute, human childhood (short-lived plasticity)

In the AI community, an often made joke is that "*... is all you need*" (as popularised by the 2017 landmark [Attention Is All You Need paper](https://arxiv.org/abs/1706.03762)). Along with attention, *scale* is often used here. Scale refers to the *level* of compute (computational resources) used - and often for how long too. And so *roughly* the size (number of parameters) of the model this compute was used to train. However, some are more cost-effective (*better*) for the same parameter count - but often at a greater training, fine-tuning, RLHF cost. [artificialanalysis.ai](https://artificialanalysis.ai/models) has great interactive visualisations for this, to better this concept - not affiliated.

So what does "*scale is all you need*" really imply? That maybe we can go most (if not all the way) to AGI[^1] by awaiting on [Moore's Law](https://wikipedia.org/wiki/Moore%27s_law) to enlighten us with more, and more compute. This concept is not entirely foolish, after all it has exponentially driven us to this point already. None of these *miracles* would be feasible without adequate structure. Raising another interesting paradox (if not enough already), how the actual code/program for AGI[^1] may be comparatively simple compared to the necessary hardware (the prior suggested by [John Carmack on the Lex Fridman podcast](https://www.youtube.com/watch?v=xLi83prR5fg)). A comparison can be drawn between the redundancy of genetically *identical* information (as DNA) in a human - undoubtedly a meticulous tapestry of nature, however without the billions of protein *workers* (computationally [parallel](https://wikipedia.org/wiki/Parallel_computing) [daemons](https://wikipedia.org/wiki/Daemon_(computing))) there would be **no** intelligence to interact with the world. This scale, whether it be intricate protein structures, or transistor density may be how intelligence is forged. [Emergent properties](https://www.britannica.com/science/emergent-property) can be found in both of these worlds: biological and artificial. With the latter borrowing this term. *Simple* building-blocks can form complex systems, binary neurons can make complex decisions - could they also *be*?

More similarities can be correlated here, on the basis of training. 

Cation. We face a major challenge here, forcing scale does **not** mandate more *intelligence* too, and if it does it may only be in some metrics - we have seen plateauing improvements since GPT-4 **so far** (pre-o1, and alike models/pipelines). Simply, current (public) LLMs[^3] find great difficulty in certain tasks, even if generalisation is currently winning.

> I use "models/pipelines" as even if OpenAI's o1-preview model was a 'quantum leap' in performance thanks to test time compute (thinking), Anthropic's **new** Claude 3.5 Sonnet achieves near-parity on [SimpleBench](https://simple-bench.com/) (suggesting at an effective data pipeline, along with simply more pre-training compute) - assuming no (foul) data contamination in favour of benchmarks.

At its foundations though, this questions evolution and the Fermi Paradox. Why is it that Elephants, who have [significantly more neurons by count](https://wikipedia.org/wiki/List_of_animals_by_number_of_neurons) are *less intelligent* than us (humans)? 

Evolution.

Environmental pressures and mutations that **favoured** more nuanced intelligence won. If we consider (Earth's) symbiotic, interdependent biospheres we find the dynamic ruthlessness of survival-of-the-fittest. Solely, (non-instinctual) intelligence itself is a gamble, intrinsically it does not provide an upper-hand immediately. If you (yes you) faced a lion with your bare body would you win? Most likely not. 

So am I implying that lion is *stupid*, and we are *weak*? **No**, but almost - intelligence is not a binary; we are both apt: the lion is smart, but we are *smarter*. In the sense of our extension of (traditional) evolution, technology can (and sadly is) *exhausting* lions. It assists to imagine evolution as a continuum over possible *mutation space*, not bound by biology. We are **not** the absolute maximum of intelligence; **we are the current local maxima in our hyperplane of mutation space**. I take heavy inspiration from [Donald Hoffman](https://wikipedia.org/wiki/Donald_D._Hoffman)'s FBT (Fitness-Beats-Truth) and ITP (Interface Theory of Perception) for these concepts - I see a logical extension from [Darwin](https://wikipedia.org/wiki/Charles_Darwin)'s theories here. We share this *interface* plane with computers, for example: *calculators* have long been superhuman at arithmetic. Yet (still, even now) [faster computers can still be made](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing) - it is how we compare continuous capabilities that informs our question on scale.

Our *newly-developed* [neocortex](https://wikipedia.org/wiki/Neocortex) was a risky toy to prioritise, especially for the odds of survival-of-the-fittest. Albeit still complex, Elephant's pale in comparison compared to humans here. So how can we summarise all this?

Higher sentient-like intelligence (in mutation space) is not (close to) guaranteed, even if the universe *favours* its entropy-accelerating curiosity. The Fermi paradox demonstrates the latter part evolution: primitive life may be abundant (in the universe), but increasing structural complexity (so introducing emergent properties) is **not straight-forward** - requiring **very favourable** conditions, for its **payoff** functions to be probabilistically supporting survival.

Some organised structures of neural networks are more appropriate *here*, some more *there*. All subject to evolution. That is why scale **may not** be *all you need*.

Intelligence is not always the *best immediate* or **optimal** solution, more often it takes *less obvious* paths to heuristic resolution. Finding non-reflexive nuanced patterns out of descended (problem-space) local minima - an experiment without guarantees, a risk for **a more fit** interface. The prior being of particular public suspicion due to rumoured [OpenAI Q* reasoning developments](https://www.youtube.com/watch?v=6_v9Ogi7P6Q). Interesting, yet also plagued by hype due to coinciding with [Sam Altam being ousted from OpenAI](https://openai.com/blog/openai-announces-leadership-transition) - inevitably spawning a myriad of theories.

> Reflecting on this (pun not intended), the aforementioned *project Strawberry* manifested as [OpenAI's o1 model](https://openai.com/o1/). The dawn of ['Level 2 reasoners'](https://www.forbes.com/sites/jodiecook/2024/07/16/openais-5-levels-of-super-ai-agi-to-outperform-human-capability/). Lately (late 2024) there has been growing speculation 'deep learning is hitting a wall', I believe [Ilya Sutskever](https://wikipedia.org/wiki/Ilya_Sutskever) (arguably *the* visionary of scale, as a co-founder of OpenAI) along with his new start-up venture [SSI](https://ssi.inc/) has a clue against this: ["Everyone just says scaling hypothesis. Everyone neglects to ask, what are we scaling?"](https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/), so what are we scaling? Since the release of the transformer architecture, pre-training has been aggressively scaled for LLMs[^3], but even with all its wonders we can take a *page* from neuroscience here - inference, the actual *thinking* procedure, requires scaling **too** due to fundamental constrictions on *fitting* to every generalisation (task); you cannot *memorise* *everything*, hence why even simply appending "think step-by-step" in a prompt can already help. [Noam Brown](https://noambrown.github.io/) is a researcher at the cutting-edge of this focus, in a [talk at Paul G. Allen School](https://www.youtube.com/watch?v=eaAonE58sLU) he highlighted the "surprising" effectiveness in explicitly [finite games](https://wikipedia.org/wiki/Finite_game) (mainly Poker) at scaling inference-time compute (to a degree), and explains the extension of this to LLMs[^3] by sampled-generator/verifier reinforcement - which was *likely* involved in o1's creation. So rather ["In three words: deep learning worked"](https://ia.samaltman.com/), just the [Perilous Venture](https://www.youtube.com/watch?v=SEkGLj0bwAU) is not over quite yet. Another factor some overlook is that benchmark saturation *should* follow a logarithmic trend, whilst mistakingly postulating this as a greater indicator of 'diminishing returns' *across the board* - score-bounded benchmarks have a maximum that is stochastically asymptotically approached with performance improvements due to: imperfections in *the* **approximator** of the process (found in the ANN[^5] of the model) - well in the *standard* error rate (effectively pushing against probabilistically achieving *perfection*), and exponentially decaying likelihood *new* data corrects *faults* in the **approximator** (that is adjusting parameters: weights, inter-node edges, and possibly [activation functions](https://arxiv.org/abs/2404.19756)). Moreover, both of aforementioned *inconveniences* scale with the [Shannon entropy](https://wikipedia.org/wiki/Entropy_(information_theory)) (a measure of complexity) associated with the task *at hand*; tic-tac-toe has far fewer symbol permutations (game positions) comparatively than language (infinite if unbounded), so its ANN[^5] **approximator** likely perfected with *little* pre-training alone whereas the latter (and world modelling) may *never* be perfected.

---

## Reason

> Not to be confused with **reasoning**, although the latter may lead to the prior.


Maybe attempts to solve the hard problem of consciousness are inevitably doomed - and beyond our mere mortal interface of *reality* (whatever that reality may be) that is stochastically naturally and or intentionally designed with a limiting dualism - one of the external world, and one of *our* own solipsistic consciousness. Although insane - how could I ever prove of any subjective experience other than **my** own - **my** interface only constitutes of **my** experiences. I wish to believe this line of reasoning of pseudo-proof by contradiction will (hopefully) not manifest into an impenetrable wall. A wall so fundamental as to eternally mystify reason for our existence.

The omnipotent-like presence of this wall does not eliminate high-level, abstracted reason to our being however - as this to our lives is rather meaning. **Meaning** is a deeply-subjective, heuristic (best-fit) resolution to the troubles of an individual's life, even if often remedied by *meaningful* social interactions that is composed of multiple beings, but nonetheless a distinct disconnect (i.e. between your brain and someone elses'). As it is the *meaningful* experiences through-out life that enrich our knitted meaning, such as engaging with loved ones. Purposeful connections prove depth to sparseness in an overwhelmingly immense world - of both artificial and natural information/data.

Of course, level of satisfaction and ease from our own *multi-modal* conversations with fellow humans (through e.g. speech, body language, touch) varies with our own personalities, for example whether: extroverts ambiverts, or introverts. Of which for the latter, connections may be more inanimate, abstract, and distanced. But nonetheless serve to ground our interface of reality to helpful goals. There is a reason why questioning reason, existence and reality embed an existentialism. After all, is it particularly useful to being human? High-level errors cause delusions from imposter syndrome, to the Dunning-Kruger effect. But all elicit a response, adjustable as a learning activity.

Even if ultimate reason is doomed, meaning is not. As juxtaposingly, reason should provide low-level fundamental basis to existence - whereas grounding understanding and satisfying beliefs provides an alluring meaning, to a fulfilling life. A coping mechanism among the unexplainable? 

And furthermore, just because we may never understand the yet-theorised intricacies of physics (and so by deductive extension reason), does not imply these *world-models* are not useful. In fact, the contrast is true - even if creating divided schools of though - the real-world applications of these technologies has build our societies, our experiences, our collective meaning.

Therefore, even if: the hard problem of **consciousness** is dead and **reason** is obfuscated, future assumed AGI [^1] / ASI [^2] systems may enlighten our approximations of this fundamental reality - by turn profoundly impacting the *sub-class* that is meaning from greater reason.

Quite sensibly, many are weary of these speculative discussions - as they hold genuine consequences for our philosophical ontological prospects. These impacts are beginning to be discussed with greater interest, by questioning 'what **purpose** will be found in post-scarcity/instrumental socio-economic futures, when *everything is solved*?'. Maybe it is the trials and tribulations that secure **meaning**. As even with significant improvements to (quantitative) median quality of life since the industrial revolution, has self-meaning *really* improved? Dissociation has grown along with zero-sum connections (often a criticism of the social media/networks). We may be more intelligent by accessible information than ever before, but has this enriched meaning?

Will reason enrich meaning? Or spell existential dread?

Reason and meaning are not directly proportional. Evolution serves no purpose to gleefully comprehend fundamental reality - beyond our knitted interface (as proposed by Fitness-Beats-Truth). Natural evolution has driven this morbid curiosity (of greater reason) to nill, whereas fitness is successfully serving its purpose.

Complex organisms such as humans, need to be as efficiently inefficient as possible to increase entropy. All roads lead to the plateau of a gradient: from wind, to diffused perfume, to **us**. Fitness has been *favoured* (naturally selected) as to dissipate energy.

Giving a **purpose** of **meaning**, not of **reason**.

Ultimately asking, how does one quantify consciousness? Does it require human-like reasoning, attention, qualia (experiences)? Or is the assumption that these can be reduced enough (i.e. to strip life to determinism, not necessarily to be confused with [free-will](https://wikipedia.org/wiki/Compatibilism)) to just "*call it a day*" - and ask ourselves the [duck test](https://wikipedia.org/wiki/Duck_test):

After all, "If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck" - otherwise the [Turing Test](https://wikipedia.org/wiki/Turing_test). Beyond (quantitative) benchmarks, our first glance impression at how *noteworthy* these models are (of consciousness) rests on their subjective **liveliness** - the apparent magnitude/rate of local agency on information from the subject's [causal cone](https://wikipedia.org/wiki/Light_cone) (context), driven by [layered, convoluted polysemantic](https://transformer-circuits.pub/2023/monosemantic-features/index.html) reward. Even in that description I could not help excluding *elaborate* jargon, but its breadth should offer adequate room for prejudice. This inherent subjectivity arises from a thirst for (currently) unavailable empiricism, phenomenology (the philosophy of experience) **seemingly** impedes rationality here due to *disallowed* circular reasoning as inhibited by the interface of the agent, and you as a human cannot experience as though through another agent's mind *yet*. Therefore satisfying neither: **rationality** (logic), nor **empiricism** (experience).

> On a tangent, I find bewildering comfort in the allegory of Plato's cave for this: if an agent was imprisoned inside a cave from birth, excluded from the *outside* world (at most experiencing casted shadows), then could that agent effectively reason about the *external* world? This is frequently applied to LLMs[^3] where language as an interface (modality) could be a limit for the *richness* required for world modelling. Instead I propose that this thought experiment is better suited to address the aforementioned conflict in phenomenology: **rationality** is *useless* without **empiricism**, an agent must observe a **stream of information** from *fundamental* reality (one way or another, including language) to reason on the basis of it at all. Context is required. Shadows may be lacking rich nuances, but they have a **quality** of their own - inhibitive but not ultimately disabling. *Paying* **attention** to complexities available, no matter how comparatively mundane, [approximates](https://wikipedia.org/wiki/Universal_approximation_theorem) their processes - optimising this approximation however is not guaranteed to be efficient (due to descended local minima). Tying into the hope of [synthetic data](https://wikipedia.org/wiki/Synthetic_data), which should serve to summarise *grounded* data - some may fear [The Bitter Lesson](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf) here, I see it as a blessing. **Liveliness** is subjective proportional to the degree our (human) perspectives of the same underlying reality differ - the information in each of our [casual cones](https://wikipedia.org/wiki/Light_cone) differing.

> I use "magnitude/rate", as to qualify these loose criteria through the *eyes* of a contemporary (median) human there is a requirement of fitting familiarity with our experiences. Explaining why even available models are [convincing us of their potential sentience](https://github.com/daveshap/Claude_Sentience) (notably Anthropic's Claude 3 Opus *on release*), even earlier robots *embodied* telltale indications of emotions as according to Hinton with his [appearance on Sana](https://www.youtube.com/watch?v=n4IQOBka8bc&t=1840s). Whilst we battle with judging familiarity for **liveliness**, future humans may (from birth) learn to interpret ["magic intelligence in the sky"](https://x.com/sama/status/1834351981881950234) as trivially similar - an undoubtedly **lively** crewmate, *just* another agent.

Its ambitions neither totally ambiguous, nor effortlessly transparent, - but undoubtedly coordinated with [enthusiasm](https://dictionary.cambridge.org/dictionary/english/lively).

Its processes neither too random, nor structured - but teetering on the [Edge of Chaos](https://arxiv.org/abs/2410.02536).

---

## Appendix

### Definitions

* [^1] : AGI (Artificial General Intelligence) - an AI system capable of performing (**cognitive**) tasks to the ability of a **median** (untrained) human
* [^2] : ASI (Artificial Super-Intelligence) - an AI system surpassing **all** human intelligence (by cognitive ability)
* [^3] : LLM (Large Language Model) - a large (number of **parameters**) AI model for **generating** and **classifying** general-purpose language, given input
* [^4] : NN (Neural Net) - an interconnected population of neurons (whether biological or artificial)
* [^5] : ANN (Artificial Neural Net) - a neural net which is artificial, not of natural origin (e.g. made by humans)

<script src="https://platform.twitter.com/widgets.js"></script>
